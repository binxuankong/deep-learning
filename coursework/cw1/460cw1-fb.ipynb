{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 1: ML basics and fully-connected networks\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "Please submit a version of this notebook containing your answers on CATe as *CW1*. Write your answers in the cells below each question.\n",
    "\n",
    "We recommend that you work on the Ubuntu workstations in the lab. This assignment and all code were only tested to work on these machines. In particular, we cannot guarantee compatibility with Windows machines and cannot promise support if you choose to work on a Windows machine.\n",
    "\n",
    "You can work from home and use the lab workstations via ssh (for list of machines: https://www.doc.ic.ac.uk/csg/facilities/lab/workstations). \n",
    "\n",
    "Once logged in, run the following commands in the terminal to set up a Python environment with all the packages you will need.\n",
    "\n",
    "    export PYTHONUSERBASE=/vol/bitbucket/nuric/pypi\n",
    "    export PATH=/vol/bitbucket/nuric/pypi/bin:$PATH\n",
    "\n",
    "Add the above lines to your `.bashrc` to have these enviroment variables set automatically each time you open your bash terminal.\n",
    "\n",
    "Any code that you submit will be expected to run in this environment. Marks will be deducted for code that fails to run.\n",
    "\n",
    "Run `jupyter-notebook` in the coursework directory to launch Jupyter notebook in your default browser.\n",
    "\n",
    "DO NOT attempt to create a virtualenv in your home folder as you will likely exceed your file quota.\n",
    "\n",
    "**DEADLINE: 7pm, Tuesday 5th February, 2019**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "1. Describe two practical methods used to estimate a supervised learning model's performance on unseen data. Which strategy is most commonly used in most deep learning applications, and why?\n",
    "2. Suppose that you have reason to believe that your multi-layer fully-connected neural network is overfitting. List four things that you could try to improve generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\**ANSWERS FOR PART 1 IN THIS CELL*\\*\n",
    "1. One method is to use split dataset into training, validation and testing set. The model is trained on the training set and the hyperparameters of the model are tuned using the validation set. The performance of the model on unseen data can then be estimated using the testing set. The other method is the cross validation method, where the dataset is split into k-partitions. The model is trained on all of the partitions except one that acts as the testing set. This process is then repeated, creating k-different models. The performance of the model on unseen data can be estimated by taking the average performance of all k models. The training and testing data split is more commonly used in deep learning applications as it is too computational costly to train k different models. Cross validation is only efficient when the dataset is small and if the model is not too complex.\n",
    "2. Weight Decay - Penalizes large weight parameters using regularization term (L1 or L2 penalty)\n",
    "   Adding Noise to Input - Adds Gaussian noise to inputs, the variance of the noise will act like a weight decay\n",
    "   Early Stopping - Stops the learning before reaching convergence (before the performance of model starts to degrade/starts to overfit)\n",
    "   Dropout - Randomly disable some neurons during training to prevents the network from being too dependent on any one of the neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adding noise to input -> one type of data augmentation. Otherwise good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "1. Why can gradient-based learning be difficult when using the sigmoid or hyperbolic tangent functions as hidden unit activation functions in deep, fully-connected neural networks?\n",
    "2. Why is the issue that arises in the previous question less of an issue when using such functions as output unit activation functions, provided that an appropriate loss function is used?\n",
    "3. What would happen if you initialize all the weights to zero in a multi-layer fully-connected neural network and attempt to train your model using gradient descent? What would happen if you did the same thing for a logistic regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\**ANSWERS FOR PART 2 IN THIS CELL*\\*\n",
    "\n",
    "1. Sigmoid and hyperbolic tangent functions can cause vanishing gradient problem, which happens when the gradient becomes too small (approaching zero), preventing the weights from changing their values.\n",
    "2. At the output layer, the gradient of the layer is calculated together with the activation function and the loss function. Hence if an appropriate loss function is used, the gradient might not be too small to cause the vanishing gradient problem.\n",
    "3. If all the weights are initialized to zero, the network will perform poorly as all of the weights will be similar. The weights are updated by gradient descent, which is done through backpropagation. The errors backpropagated are proportional to the value fo the weights. If all the weights are zero, the backpropagated errors will be the same, causing all the weights to be updated to the same value. For logistic regression, there is no issue for initializing all weights to zero as the update rule is simple compared to that of a neural network. As logistic regression is a convex problem, convergence is guaranteed regardless of starting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.2:\n",
    "    - Using for e.g. log loss with sigmoid/tanh at output layer will undo the exponentiation in the latter, resulting in a non-negligible gradient being propagated back to earlier layers.\n",
    "\n",
    "The facts mentioned for 2.3 are correct, but something is missing:\n",
    "    - Regarding NNs: The fact that all weights are zero, means that all hidden units after first layer will be zero, regardless of the input. All corresponding weights will have gradient zero and as such will not change.\n",
    "    - Regarding logistic reg.: The gradients of the parameters of the single layer, do, in fact, depend on the input. It is not the starting point that matters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "In this part, you will use PyTorch to implement and train a multinomial logistic regression model to classify MNIST digits.\n",
    "\n",
    "Restrictions:\n",
    "* You must use (but not modify) the code provided in `utils.py`. **This file is deliberately not documented**; read it carefully as you will need to understand what it does to complete the tasks.\n",
    "* You are NOT allowed to use the `torch.nn` module.\n",
    "\n",
    "Please insert your solutions to the following tasks in the cells below:\n",
    "1. Complete the `MultinomialLogisticRegressionClassifier` class below by filling in the missing parts (expected behaviour is prescribed in the documentation):\n",
    "    * The constructor\n",
    "    * `forward`\n",
    "    * `parameters`\n",
    "    * `l1_weight_penalty`\n",
    "    * `l2_weight_penalty`\n",
    "\n",
    "2. The default hyperparameters for `MultilayerClassifier` and `run_experiment` have been deliberately chosen to produce poor results. Experiment with different hyperparameters until you are able to get a test set accuracy above 92% after a maximum of 10 epochs of training. However, DO NOT use the test set accuracy to tune your hyperparameters; use the validation loss / accuracy. You can use any optimizer in `torch.optim`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    log_softmax = torch.empty(x.shape)\n",
    "    logsumexp = 0\n",
    "    for i in range(x.shape[0]):\n",
    "        b = x[i].max()\n",
    "        y = np.exp(x[i] - b)\n",
    "        logsumexp = b + np.log(y.sum())\n",
    "        log_softmax[i] = x[i] - logsumexp\n",
    "    return log_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *CODE FOR PART 3.1 IN THIS CELL*\n",
    "\n",
    "class MultinomialLogisticRegressionClassifier:\n",
    "    def __init__(self, weight_init_sd=100.0):\n",
    "        \"\"\"\n",
    "        Initializes model parameters to values drawn from the Normal\n",
    "        distribution with mean 0 and standard deviation `weight_init_sd`.\n",
    "        \"\"\"\n",
    "        self.weight_init_sd = weight_init_sd\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        n = torch.distributions.normal.Normal(torch.tensor([0.0]), torch.tensor([self.weight_init_sd]))\n",
    "        self.W = (n.sample((784,10))).view(784,10)\n",
    "        self.W.requires_grad_()\n",
    "        self.b = torch.zeros(10).view(10)\n",
    "        self.b.requires_grad_()\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Performs the forward pass through the model.\n",
    "        \n",
    "        Expects `inputs` to be a Tensor of shape (batch_size, 1, 28, 28) containing\n",
    "        minibatch of MNIST images.\n",
    "        \n",
    "        Inputs should be flattened into a Tensor of shape (batch_size, 784),\n",
    "        before being fed into the model.\n",
    "        \n",
    "        Should return a Tensor of logits of shape (batch_size, 10).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        h = torch.add(torch.mm(inputs, self.W), self.b)\n",
    "        # Could not get own log_softmax function to work\n",
    "        return F.log_softmax(h, dim=1)\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Should return an iterable of all the model parameter Tensors.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        return [self.W, self.b]\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        \n",
    "    def l1_weight_penalty(self):\n",
    "        \"\"\"\n",
    "        Computes and returns the L1 norm of the model's weight vector (i.e. sum\n",
    "        of absolute values of all model parameters).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        l1_reg = 0.\n",
    "        for param in self.W:\n",
    "            l1_reg += torch.sum(torch.abs(param))\n",
    "        l1_reg += torch.sum(torch.abs(self.b))\n",
    "        return l1_reg\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def l2_weight_penalty(self):\n",
    "        \"\"\"\n",
    "        Computes and returns the L2 weight penalty (i.e. \n",
    "        sum of squared values of all model parameters).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        l2_reg = 0.\n",
    "        for param in self.W:\n",
    "            l2_reg += torch.sum(torch.pow(param, 2))\n",
    "        l2_reg += torch.sum(torch.pow(self.b, 2))\n",
    "        return l2_reg\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: training...\n",
      "Train set:\tAverage loss: 0.5942, Accuracy: 0.8590\n",
      "Validation set:\tAverage loss: 0.4158, Accuracy: 0.8888\n",
      "\n",
      "Epoch 1: training...\n",
      "Train set:\tAverage loss: 0.3841, Accuracy: 0.8960\n",
      "Validation set:\tAverage loss: 0.3644, Accuracy: 0.8992\n",
      "\n",
      "Epoch 2: training...\n",
      "Train set:\tAverage loss: 0.3515, Accuracy: 0.9028\n",
      "Validation set:\tAverage loss: 0.3429, Accuracy: 0.9060\n",
      "\n",
      "Epoch 3: training...\n",
      "Train set:\tAverage loss: 0.3347, Accuracy: 0.9068\n",
      "Validation set:\tAverage loss: 0.3284, Accuracy: 0.9078\n",
      "\n",
      "Epoch 4: training...\n",
      "Train set:\tAverage loss: 0.3240, Accuracy: 0.9104\n",
      "Validation set:\tAverage loss: 0.3205, Accuracy: 0.9117\n",
      "\n",
      "Epoch 5: training...\n",
      "Train set:\tAverage loss: 0.3163, Accuracy: 0.9122\n",
      "Validation set:\tAverage loss: 0.3150, Accuracy: 0.9147\n",
      "\n",
      "Epoch 6: training...\n",
      "Train set:\tAverage loss: 0.3105, Accuracy: 0.9139\n",
      "Validation set:\tAverage loss: 0.3097, Accuracy: 0.9142\n",
      "\n",
      "Epoch 7: training...\n",
      "Train set:\tAverage loss: 0.3060, Accuracy: 0.9153\n",
      "Validation set:\tAverage loss: 0.3068, Accuracy: 0.9152\n",
      "\n",
      "Epoch 8: training...\n",
      "Train set:\tAverage loss: 0.3025, Accuracy: 0.9160\n",
      "Validation set:\tAverage loss: 0.3038, Accuracy: 0.9160\n",
      "\n",
      "Epoch 9: training...\n",
      "Train set:\tAverage loss: 0.2992, Accuracy: 0.9170\n",
      "Validation set:\tAverage loss: 0.2998, Accuracy: 0.9158\n",
      "\n",
      "Test set:\tAverage loss: 0.2885, Accuracy: 0.9209\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# *CODE FOR PART 3.2 IN THIS CELL - EXAMPLE WITH DEFAULT PARAMETERS PROVIDED *\n",
    "model = MultinomialLogisticRegressionClassifier(weight_init_sd=0.01)\n",
    "res = run_experiment(\n",
    "    model,\n",
    "    optimizer=optim.SGD(model.parameters(), 0.05),\n",
    "    train_loader=train_loader_0,\n",
    "    val_loader=val_loader_0,\n",
    "    test_loader=test_loader_0,\n",
    "    n_epochs=10,\n",
    "    l1_penalty_coef=0.00005,\n",
    "    l2_penalty_coef=0.00005,\n",
    "    suppress_output=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters used to get a test set accuracy of 92.09% are:\n",
    "\n",
    "weight_init_sd = 0.01\n",
    "\n",
    "optimizer = SGD\n",
    "\n",
    "learning_rate = 0.05\n",
    "\n",
    "l1_penalty_coef = 0.00005\n",
    "\n",
    "l2_penalty_coef = 0.00005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Great code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4\n",
    "\n",
    "In this part, you will use PyTorch to implement and train a multi-layer fully-connected neural network to classify MNIST digits.\n",
    "\n",
    "Your network must have three hidden layers with 128, 64, and 32 hidden units respectively.\n",
    "\n",
    "The same restrictions as in Part 3 apply.\n",
    "\n",
    "Please insert your solutions to the following tasks in the cells below:\n",
    "1. Complete the `MultilayerClassifier` class below by filling in the missing parts of the following methods (expected behaviour is prescribed in the documentation):\n",
    "\n",
    "    * The constructor\n",
    "    * `forward`\n",
    "    * `parameters`\n",
    "    * `l1_weight_penalty`\n",
    "    * `l2_weight_penalty`\n",
    "\n",
    "2. The default hyperparameters for `MultilayerClassifier` and `run_experiment` have been deliberately chosen to produce poor results. Experiment with different hyperparameters until you are able to get a test set accuracy above 97% after a maximum of 10 epochs of training. However, DO NOT use the test set accuracy to tune your hyperparameters; use the validation loss / accuracy. You can use any optimizer in `torch.optim`.\n",
    "\n",
    "3. Describe an alternative strategy for initializing weights that may perform better than the strategy we have used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *CODE FOR PART 4.1 IN THIS CELL*\n",
    "\n",
    "class MultilayerClassifier:\n",
    "    def __init__(self, activation_fun=\"sigmoid\", weight_init_sd=1.0):\n",
    "        \"\"\"\n",
    "        Initializes model parameters to values drawn from the Normal\n",
    "        distribution with mean 0 and standard deviation `weight_init_sd`.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.activation_fun = activation_fun\n",
    "        self.weight_init_sd = weight_init_sd\n",
    "\n",
    "        if self.activation_fun == \"relu\":\n",
    "            self.activation = F.relu\n",
    "        elif self.activation_fun == \"sigmoid\":\n",
    "            self.activation = torch.sigmoid\n",
    "        elif self.activation_fun == \"tanh\":\n",
    "            self.activation = torch.tanh\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        n = torch.distributions.normal.Normal(torch.tensor([0.0]), torch.tensor([self.weight_init_sd]))\n",
    "        self.W1 = n.sample((784,64)).view(784,64)\n",
    "        self.W1.requires_grad_()\n",
    "        self.b1 = torch.zeros(64).view(64)\n",
    "        self.b1.requires_grad_()\n",
    "        self.W2 = n.sample((64,32)).view(64,32)\n",
    "        self.W2.requires_grad_()\n",
    "        self.b2 = torch.zeros(32).view(32)\n",
    "        self.b2.requires_grad_()\n",
    "        self.W3 = n.sample((32,10)).view(32,10)\n",
    "        self.W3.requires_grad_()\n",
    "        self.b3 = torch.zeros(10).view(10)\n",
    "        self.b3.requires_grad_()\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Performs the forward pass through the model.\n",
    "        \n",
    "        Expects `inputs` to be Tensor of shape (batch_size, 1, 28, 28) containing\n",
    "        minibatch of MNIST images.\n",
    "        \n",
    "        Inputs should be flattened into a Tensor of shape (batch_size, 784),\n",
    "        before being fed into the model.\n",
    "        \n",
    "        Should return a Tensor of logits of shape (batch_size, 10).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        h = torch.add(torch.mm(inputs, self.W1), self.b1)\n",
    "        h = torch.add(torch.mm(h, self.W2), self.b2)\n",
    "        h = torch.add(torch.mm(h, self.W3), self.b3)\n",
    "        return F.log_softmax(h, dim=1)\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Should return an iterable of all the model parameter Tensors.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        return [self.W1, self.b1, self.W2, self.b2, self.W3, self.b3]\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        \n",
    "    \n",
    "    def l1_weight_penalty(self):\n",
    "        \"\"\"\n",
    "        Computes and returns the L1 norm of the model's weight vector (i.e. sum\n",
    "        of absolute values of all model parameters).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        l1_reg = 0.\n",
    "        for param in self.W1:\n",
    "            l1_reg += torch.sum(torch.abs(param))\n",
    "        for param in self.W2:\n",
    "            l1_reg += torch.sum(torch.abs(param))\n",
    "        for param in self.W3:\n",
    "            l1_reg += torch.sum(torch.abs(param))\n",
    "        l1_reg += torch.sum(torch.abs(self.b1))\n",
    "        l1_reg += torch.sum(torch.abs(self.b2))\n",
    "        l1_reg += torch.sum(torch.abs(self.b3))\n",
    "        return l1_reg\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n",
    "\n",
    "    def l2_weight_penalty(self):\n",
    "        \"\"\"\n",
    "        Computes and returns the L2 weight penalty (i.e. \n",
    "        sum of squared values of all model parameters).\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        #                       ** START OF YOUR CODE **\n",
    "        #######################################################################\n",
    "        l2_reg = 0.\n",
    "        for param in self.W1:\n",
    "            l2_reg += torch.sum(torch.pow(param, 2))\n",
    "        for param in self.W2:\n",
    "            l2_reg += torch.sum(torch.pow(param, 2))\n",
    "        for param in self.W3:\n",
    "            l2_reg += torch.sum(torch.pow(param, 2))\n",
    "        l2_reg += torch.sum(torch.pow(self.b1, 2))\n",
    "        l2_reg += torch.sum(torch.pow(self.b2, 2))\n",
    "        l2_reg += torch.sum(torch.pow(self.b3, 2))\n",
    "        return l2_reg\n",
    "        #######################################################################\n",
    "        #                       ** END OF YOUR CODE **\n",
    "        #######################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: training...\n",
      "Train set:\tAverage loss: 1.3339, Accuracy: 0.5462\n",
      "Validation set:\tAverage loss: 0.4857, Accuracy: 0.8603\n",
      "\n",
      "Epoch 1: training...\n",
      "Train set:\tAverage loss: 0.4217, Accuracy: 0.8783\n",
      "Validation set:\tAverage loss: 0.3706, Accuracy: 0.8933\n",
      "\n",
      "Epoch 2: training...\n",
      "Train set:\tAverage loss: 0.3595, Accuracy: 0.8994\n",
      "Validation set:\tAverage loss: 0.3402, Accuracy: 0.9050\n",
      "\n",
      "Epoch 3: training...\n",
      "Train set:\tAverage loss: 0.3329, Accuracy: 0.9069\n",
      "Validation set:\tAverage loss: 0.3274, Accuracy: 0.9087\n",
      "\n",
      "Epoch 4: training...\n",
      "Train set:\tAverage loss: 0.3153, Accuracy: 0.9109\n",
      "Validation set:\tAverage loss: 0.3050, Accuracy: 0.9178\n",
      "\n",
      "Epoch 5: training...\n",
      "Train set:\tAverage loss: 0.3043, Accuracy: 0.9144\n",
      "Validation set:\tAverage loss: 0.2995, Accuracy: 0.9145\n",
      "\n",
      "Epoch 6: training...\n",
      "Train set:\tAverage loss: 0.2952, Accuracy: 0.9167\n",
      "Validation set:\tAverage loss: 0.2911, Accuracy: 0.9207\n",
      "\n",
      "Epoch 7: training...\n",
      "Train set:\tAverage loss: 0.2895, Accuracy: 0.9195\n",
      "Validation set:\tAverage loss: 0.2887, Accuracy: 0.9202\n",
      "\n",
      "Epoch 8: training...\n",
      "Train set:\tAverage loss: 0.2833, Accuracy: 0.9199\n",
      "Validation set:\tAverage loss: 0.2830, Accuracy: 0.9218\n",
      "\n",
      "Epoch 9: training...\n",
      "Train set:\tAverage loss: 0.2798, Accuracy: 0.9213\n",
      "Validation set:\tAverage loss: 0.2859, Accuracy: 0.9200\n",
      "\n",
      "Test set:\tAverage loss: 0.2759, Accuracy: 0.9211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# *CODE FOR PART 4.2 IN THIS CELL - EXAMPLE WITH DEFAULT PARAMETERS PROVIDED *\n",
    "\n",
    "model = MultilayerClassifier(activation_fun='sigmoid', weight_init_sd=0.02)\n",
    "res = run_experiment(\n",
    "    model,\n",
    "    optimizer=optim.SGD(model.parameters(), 0.05),\n",
    "    train_loader=train_loader_0,\n",
    "    val_loader=val_loader_0,\n",
    "    test_loader=test_loader_0,\n",
    "    n_epochs=10,\n",
    "    l1_penalty_coef=0.00001,\n",
    "    l2_penalty_coef=0.00001,\n",
    "    suppress_output=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters used to get a test set accuracy of 92.11% are:\n",
    "\n",
    "weight_init_sd = 0.02\n",
    "\n",
    "optimizer = SGD\n",
    "\n",
    "learning_rate = 0.05\n",
    "\n",
    "l1_penalty_coef = 0.00001\n",
    "\n",
    "l2_penalty_coef = 0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\**ANSWERS FOR PART 4.3 IN THIS CELL*\\*\n",
    "\n",
    "An alternative strategy for initializing weights is to initialize the weights randomly, but differing in range depending on the size of the previous layer of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Great code.\n",
    "\n",
    "As for 4.3: Perhaps you refer at the He et al. 2015 initialisation?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
